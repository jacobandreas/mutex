{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.utils.data as torch_data\n",
    "import torch.nn.functional as F\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import combinations, product\n",
    "from mutex import EncDec, Vocab, batch_seqs, Mutex\n",
    "from data import encode,  generate_fig2_exp, Oracle, collate, eval_format\n",
    "from absl import app, flags\n",
    "import sys\n",
    "import os\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags_dict = FLAGS._flags()\n",
    "keys_list = [keys for keys in flags_dict]\n",
    "for keys in keys_list: delattr(FLAGS,keys)\n",
    "flags.DEFINE_integer(\"dim\", 200, \"trasnformer dimension\")\n",
    "flags.DEFINE_integer(\"n_layers\", 1, \"number of rnn layers\")\n",
    "flags.DEFINE_integer(\"n_batch\", 1, \"batch size\")\n",
    "flags.DEFINE_integer(\"n_epochs\",50, \"number of training epochs\")\n",
    "flags.DEFINE_integer(\"Nsample\",100, \"number of samples from py\")\n",
    "flags.DEFINE_float(\"lr\", 0.001, \"learning rate\")\n",
    "flags.DEFINE_float(\"temp\", 1.0, \"temperature for samplings\")\n",
    "flags.DEFINE_float(\"dropout\", 0.05, \"dropout\")\n",
    "flags.DEFINE_float(\"lamda\", 0.05, \"lambda\")\n",
    "flags.DEFINE_float(\"kl_lamda\", 1.0, \"extra lambda for kl\")\n",
    "flags.DEFINE_float(\"ent\", 0.001, \"qx|y entropy\")\n",
    "flags.DEFINE_string(\"save_model\", \"model.m\", \"model save location\")\n",
    "flags.DEFINE_integer(\"seed\", 0, \"random seed\")\n",
    "flags.DEFINE_bool(\"debug\", False, \"debug mode\")\n",
    "flags.DEFINE_bool(\"regularize\", True, \"apply regularization\")\n",
    "flags.DEFINE_bool(\"full_data\", True, \"full figure 2 experiments, otherwise color matching\")\n",
    "\n",
    "import hlog\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "plt.rcParams['figure.dpi'] = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS(['mutex.ipynb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_symbols_list   = set(['dax', 'lug', 'wif', 'zup', 'fep', 'blicket', 'kiki', 'tufa', 'gazzer'])\n",
    "DEVICE = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain(model, train_dataset, val_dataset):\n",
    "    opt = optim.Adam(model.parameters(), lr=FLAGS.lr)\n",
    "\n",
    "    train_loader = torch_data.DataLoader(\n",
    "        train_dataset, batch_size=FLAGS.n_batch, shuffle=True, collate_fn=collate\n",
    "    )\n",
    "\n",
    "    best_loss  = np.inf\n",
    "\n",
    "    for i_epoch in range(2*FLAGS.n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_batches = 0\n",
    "        for inp, _ in train_loader:\n",
    "            x = inp[:-1,:]\n",
    "            pred, *extras = model(None, x.shape[0], x.to(DEVICE))\n",
    "            output = pred.view(-1, len(model.vocab))\n",
    "            loss = model.nllreduce(output,inp[1:, :].view(-1).to(DEVICE))\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_loss    += loss.item() * inp.shape[1]\n",
    "            train_batches += inp.shape[1]\n",
    "\n",
    "        if (i_epoch + 1) % 2 != 0:\n",
    "            continue\n",
    "\n",
    "        curr_loss = train_loss / train_batches\n",
    "        best_loss = min(best_loss, curr_loss)\n",
    "\n",
    "        hlog.value(\"loss\", curr_loss)\n",
    "        hlog.value(\"best loss\", best_loss)\n",
    "\n",
    "\n",
    "    hlog.value(\"best loss\", best_loss)\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_dataset, val_dataset):\n",
    "    opt = optim.Adam(model.parameters(), lr=FLAGS.lr)\n",
    "\n",
    "    train_loader = torch_data.DataLoader(\n",
    "        train_dataset, batch_size=FLAGS.n_batch, shuffle=False,\n",
    "        collate_fn=collate\n",
    "    )\n",
    "\n",
    "    best_f1  = -np.inf\n",
    "    best_acc = -np.inf\n",
    "\n",
    "    for i_epoch in range(FLAGS.n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_batches = 0\n",
    "        for inp, out in train_loader:\n",
    "            nll = model(inp.to(DEVICE), out.to(DEVICE))\n",
    "            loss = nll.mean()\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            opt.step()\n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "\n",
    "        if (i_epoch + 1) % 3 != 0 and i_epoch != FLAGS.n_epochs:\n",
    "            continue\n",
    "\n",
    "        with hlog.task(i_epoch):\n",
    "            hlog.value(\"curr loss\", train_loss / train_batches)\n",
    "            acc, f1 = validate(model, val_dataset)\n",
    "            hlog.value(\"acc\", acc)\n",
    "            hlog.value(\"f1\", f1)\n",
    "            best_f1 = max(best_f1, f1)\n",
    "            best_acc = max(best_acc, acc)\n",
    "            hlog.value(\"best_acc\", best_acc)\n",
    "            hlog.value(\"best_f1\", best_f1)\n",
    "            print()\n",
    "    torch.save(model, f\"seed_{FLAGS.seed}_\"+ FLAGS.save_model)\n",
    "\n",
    "    hlog.value(\"final_acc\", acc)\n",
    "    hlog.value(\"final_f1\", f1)\n",
    "    hlog.value(\"best_acc\", best_acc)\n",
    "    hlog.value(\"best_f1\", best_f1)\n",
    "    return acc, f1\n",
    "\n",
    "\n",
    "def validate(model, val_dataset, vis=False):\n",
    "    model.eval()\n",
    "    hlog.value(\"qxy samples\", model.sample_qxy(model.py.sample(20,model.MAXLEN_Y),temp=model.temp))\n",
    "    first = True\n",
    "    val_loader = torch_data.DataLoader(\n",
    "        val_dataset, batch_size=FLAGS.n_batch, shuffle=True,\n",
    "        collate_fn=collate\n",
    "    )\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    with torch.no_grad():\n",
    "        for inp, out in val_loader:\n",
    "            pred, _ = model.sample(inp.to(DEVICE), temp=1.0, max_len=model.MAXLEN_Y, greedy=True)\n",
    "            for i, seq in enumerate(pred):\n",
    "                ref = out[:, i].detach().cpu().numpy().tolist()\n",
    "                ref = eval_format(model.vocab_y, ref)\n",
    "                pred_here = eval_format(model.vocab_y, pred[i])\n",
    "                correct_here = pred_here == ref\n",
    "                correct += correct_here\n",
    "                tp_here = len([p for p in pred_here if p in ref])\n",
    "                tp += tp_here\n",
    "                fp_here = len([p for p in pred_here if p not in ref])\n",
    "                fp += fp_here\n",
    "                fn_here = len([p for p in ref if p not in pred_here])\n",
    "                fn += fn_here\n",
    "                total += 1\n",
    "                if vis:\n",
    "                    with hlog.task(total):\n",
    "                        hlog.value(\"label\", correct_here)\n",
    "                        hlog.value(\"tp\",tp_here)\n",
    "                        hlog.value(\"fp\",fp_here)\n",
    "                        hlog.value(\"fn\",fn_here)\n",
    "                        inp_lst = inp[:, i].detach().cpu().numpy().tolist()\n",
    "                        hlog.value(\"input\", eval_format(model.vocab_x, inp_lst))\n",
    "                        hlog.value(\"gold\", ref)\n",
    "                        hlog.value(\"pred\", pred_here)\n",
    "\n",
    "\n",
    "    acc = correct / total\n",
    "    if tp+fp > 0:\n",
    "        prec = tp / (tp + fp)\n",
    "    else:\n",
    "        prec=0\n",
    "    rec = tp / (tp + fn)\n",
    "    if prec == 0 or rec == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * prec * rec / (prec + rec)\n",
    "    hlog.value(\"acc\", acc)\n",
    "    hlog.value(\"f1\", f1)\n",
    "    return acc, f1\n",
    "\n",
    "def swap_io(items):\n",
    "    return [(y,x) for (x,y) in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    hlog.flags()\n",
    "\n",
    "    random.seed(FLAGS.seed)\n",
    "    np.random.seed(FLAGS.seed)\n",
    "    torch.manual_seed(FLAGS.seed)\n",
    "\n",
    "\n",
    "    input_symbols_list   = set(['red', 'yellow', 'green', 'blue', 'purple', 'pink', 'around', 'thrice', 'after'])\n",
    "    #input_symbols_list   = set(['dax', 'lug', 'wif', 'zup', 'fep', 'blicket', 'kiki', 'tufa', 'gazzer'])\n",
    "    output_symbols_list  = set(['RED', 'YELLOW', 'GREEN', 'BLUE', 'PURPLE', 'PINK'])\n",
    "\n",
    "    study, test = generate_fig2_exp(input_symbols_list, output_symbols_list)\n",
    "\n",
    "    vocab_x = Vocab()\n",
    "    vocab_y = Vocab()\n",
    "\n",
    "    if FLAGS.full_data:\n",
    "        for sym in input_symbols_list:\n",
    "            vocab_x.add(sym)\n",
    "        for sym in output_symbols_list:\n",
    "            vocab_y.add(sym)\n",
    "        max_len_x = 7\n",
    "        max_len_y = 9\n",
    "    else:\n",
    "        test, study  = study[3:4], study[0:3]\n",
    "        for (x,y) in test+study:\n",
    "            for sym in x:\n",
    "                vocab_x.add(sym)\n",
    "            for sym in y:\n",
    "                vocab_y.add(sym)\n",
    "        max_len_x = 2\n",
    "        max_len_y = 2\n",
    "\n",
    "    hlog.value(\"vocab_x\\n\", vocab_x)\n",
    "    hlog.value(\"vocab_y\\n\", vocab_y)\n",
    "    hlog.value(\"study\\n\", study)\n",
    "    hlog.value(\"test\\n\", test)\n",
    "\n",
    "\n",
    "    train_items, test_items = encode(study,vocab_x, vocab_y), encode(test,vocab_x, vocab_y)\n",
    "\n",
    "#   outlist = list(output_symbols_list)\n",
    "\n",
    "    oracle_py  = Oracle(train_items, test_items, DEVICE, dist=\"py\",  vocab_x=vocab_x, vocab_y=vocab_y)\n",
    "    oracle_px  = Oracle(train_items, test_items, DEVICE, dist=\"px\",  vocab_x=vocab_x, vocab_y=vocab_y)\n",
    "    oracle_qxy = Oracle(train_items, test_items, DEVICE, dist=\"qxy\", vocab_x=vocab_x, vocab_y=vocab_y)\n",
    "\n",
    "    model = Mutex(vocab_x,\n",
    "                  vocab_y,\n",
    "                  FLAGS.dim,\n",
    "                  FLAGS.dim,\n",
    "                  oracle_py,\n",
    "                  max_len_x=max_len_x,\n",
    "                  max_len_y=max_len_y,\n",
    "                  copy=False,\n",
    "                  n_layers=FLAGS.n_layers,\n",
    "                  self_att=False,\n",
    "                  dropout=FLAGS.dropout,\n",
    "                  lamda=FLAGS.lamda,\n",
    "                  kl_lamda=FLAGS.kl_lamda,\n",
    "                  Nsample=FLAGS.Nsample,\n",
    "                  temp=FLAGS.temp,\n",
    "                  regularize=FLAGS.regularize,\n",
    "                  ent=FLAGS.ent,\n",
    "                 ).to(DEVICE)\n",
    "\n",
    "    if FLAGS.regularize and not isinstance(model.px,Oracle):\n",
    "        with hlog.task(\"pretrain px\"):\n",
    "            pretrain(model.px, train_items + test_items, test_items)\n",
    "            for p in model.px.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "\n",
    "    with hlog.task(\"Initial Samples\"):\n",
    "        hlog.value(\"px samples\\n\",  \"\\n\".join(model.sample_px(20)))\n",
    "        hlog.value(\"py samples\\n\",  \"\\n\".join(model.sample_py(20)))\n",
    "        hlog.value(\"qxy debug samples\\n\", \"\\n\".join(model.sample_qxy_debug(N=20)))\n",
    "        hlog.value(\"qxy debug data\\n\", \"\\n\".join(model.sample_qxy_debug_data(train_items + test_items)))\n",
    "#         hlog.value(\"qxy samples\", \"\\n\".join(model.sample_qxy(model.py.sample(20,max_len),temp=model.temp)))\n",
    "#         hlog.value(\"qxy samples (gumbel)\", \"\\n\".join(model.sample_qxy_gumbel(model.py.sample(20,max_len),temp=model.temp)))\n",
    "\n",
    "#     if not isinstance(model.qxy,Oracle):\n",
    "#         train(model.qxy, swap_io(train_items) + swap_io(test_items), swap_io(test_items))\n",
    "#     if not isinstance(model.pyx,Oracle):\n",
    "#         train(model.pyx, train_items + test_items, test_items)\n",
    "#         for param in model.pyx.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "    with hlog.task(\"train model\"):\n",
    "        acc, f1 = train(model, train_items, test_items)\n",
    "\n",
    "    with hlog.task(\"Final Samples\"):\n",
    "        hlog.value(\"px samples\\n\", \"\\n\".join(model.sample_px(20)))\n",
    "        hlog.value(\"py samples\\n\", \"\\n\".join(model.sample_py(20)))\n",
    "        hlog.value(\"qxy debug samples\\n\", \"\\n\".join(model.sample_qxy_debug(N=20)))\n",
    "        hlog.value(\"qxy debug data\\n\", \"\\n\".join(model.sample_qxy_debug_data(train_items + test_items)))\n",
    "        hlog.value(\"qxy samples (gumbel)\\n\", \"\\n\".join(model.sample_qxy_gumbel(model.py.sample(20,max_len_y),temp=model.temp)))\n",
    "        #hlog.value(\"qxy samples\", \"\\n\".join(model.sample_qxy(model.py.sample(20,max_len),temp=model.temp)))\n",
    "\n",
    "    if FLAGS.regularize:\n",
    "        losses = pd.DataFrame(model.loss_container)\n",
    "        figure = sns.lineplot(data=losses, dashes=False).figure\n",
    "        figure.savefig(f\"{FLAGS.seed}_plot.png\")\n",
    "\n",
    "    with hlog.task(\"train evaluation\"):\n",
    "        validate(model, train_items, vis=True)\n",
    "\n",
    "    with hlog.task(\"test evaluation\"):\n",
    "        validate(model, test_items, vis=True)\n",
    "        \n",
    "    return model, train_items, test_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, study, test = main([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pqxy1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(model, test, vis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_qxy_debug(model,N=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.sample_px(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = model.py.sample(1,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cxs, _ = model.qxy.sample_with_gumbel(ys, model.max_len, temp=model.temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"qxy samples: \", model.sample_qxy(model.py.sample(1,7),temp=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"qxy samples: \", model.sample_qxy_gumbel(model.py.sample(1,7),temp=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, _ = model.qxy.sample(ys, model.max_len, temp=model.temp)\n",
    "ux = [list(x) for x in set(tuple(x) for x in xs)]\n",
    "xps   = batch_seqs(ux).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprob_px = model.px.logprob(xps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprob_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(logprob_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in ys.split(1,dim=1):\n",
    "    ybatch = y.repeat(1, xps.shape[1])\n",
    "    logprob_qxy = model.qxy.logprob(ybatch, xps)\n",
    "    print(logprob_qxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(logprob_qxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"qxy samples: \", model.sample_qxy(ys,temp=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(model, encode(test,model.vocab), vis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(model, encode(study,model.vocab), vis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.loss_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(model.loss_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(data=df, dashes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
